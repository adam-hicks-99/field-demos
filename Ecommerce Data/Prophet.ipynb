{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3601bfd-307c-49a6-92e9-44e6b53689e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install holidays==0.24\n",
    "!pip install prophet==1.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05002fe7-6028-4dd7-8150-e33a463b93ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe1b0348-6eb1-4986-a686-4e36bfd44dbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create catalog if not exists forecasting_poc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b2d9975-228d-4180-a886-f4e77143a4e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "use catalog forecasting_poc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7607661d-1c7e-4dda-a28c-23eea6f2a786",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create schema if not exists prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3872e060-2000-4de2-a638-550f84a5c761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "use prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f2484b0-fc6a-4e22-96a8-fede3bbf1d32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download the dataset directly from Prophet's GitHub repository\n",
    "url = \"https://raw.githubusercontent.com/facebook/prophet/master/examples/example_wp_log_peyton_manning.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Convert pandas DataFrame to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "# Save as table time_series_bronze\n",
    "spark_df.write.mode('overwrite').saveAsTable(\"time_series_bronze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbd95e01-216e-4a9f-8ab6-0d53ea3f7d3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from time_series_bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1352b26f-2b25-4998-860e-612cdcfbd68e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the base dataset from your bronze table\n",
    "base_df = spark.table(\"time_series_bronze\").toPandas()\n",
    "\n",
    "# Ensure the 'ds' column is in datetime format\n",
    "base_df['ds'] = pd.to_datetime(base_df['ds'])\n",
    "\n",
    "# Get today's date (normalized to remove the time component)\n",
    "current_date = pd.to_datetime(\"today\").normalize()\n",
    "\n",
    "# Calculate the shift needed so that the max date in base_df becomes today\n",
    "max_base_date = base_df['ds'].max()\n",
    "date_shift = current_date - max_base_date\n",
    "base_df['ds'] = base_df['ds'] + date_shift\n",
    "\n",
    "# Generate 1500 time series by replicating the base data with slight modifications on 'y'\n",
    "num_series = 150\n",
    "dfs = []\n",
    "\n",
    "for i in range(1, num_series + 1):\n",
    "    df_copy = base_df.copy()\n",
    "    df_copy['time_series'] = f\"time_series_{i}\"\n",
    "    # Add small random noise (mean=0, std=0.1) to slightly vary the target variable\n",
    "    df_copy['y'] = df_copy['y'] + np.random.normal(loc=0, scale=0.1, size=len(df_copy))\n",
    "    df_copy['y'] = df_copy['y'].round(2)  # Round to nearest hundredth\n",
    "    dfs.append(df_copy)\n",
    "\n",
    "# Combine all replicated series into one DataFrame\n",
    "df_multigrain = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Convert the Pandas DataFrame to a Spark DataFrame\n",
    "spark_df = spark.createDataFrame(df_multigrain)\n",
    "\n",
    "# Save the Spark DataFrame as a table called time_series_curated (overwrite if exists)\n",
    "spark_df.write.mode(\"overwrite\").saveAsTable(\"time_series_curated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a49551b6-c254-496f-ae53-078e3ad756c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from time_series_curated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e73c7b9c-ef31-46cb-b589-bc67d87bdff0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.models.signature import infer_signature\n",
    "from prophet import Prophet\n",
    "from pyspark.sql import SparkSession\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18058f6e-e7c7-41a6-a623-6d051565dcd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, concat\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "sql_statement = '''\n",
    "  SELECT\n",
    "    time_series as id,\n",
    "    CAST(ds as date) as ds,\n",
    "    SUM(y) as y\n",
    "  FROM time_series_curated\n",
    "  where ds >= '2021-01-01'\n",
    "  GROUP BY id, ds\n",
    "  ORDER BY id, ds\n",
    "  '''\n",
    "\n",
    "id_history = (\n",
    "  spark\n",
    "    .sql( sql_statement )\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42f26580-41aa-4f07-8038-e60af501a454",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    " \n",
    "result_schema =StructType([\n",
    "  StructField('ds',DateType()),\n",
    "  StructField('id',StringType()),\n",
    "  StructField('y',FloatType()),\n",
    "  StructField('yhat',FloatType()),\n",
    "  StructField('yhat_upper',FloatType()),\n",
    "  StructField('yhat_lower',FloatType())\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a32d7dc-daa7-465d-8b64-3d449d99a3a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def forecast( history_pd: pd.DataFrame ) -> pd.DataFrame:\n",
    "  history_pd = history_pd.dropna()\n",
    "  \n",
    "  # configure the model\n",
    "  model = Prophet(\n",
    "    interval_width=0.95,\n",
    "    growth='linear',\n",
    "    daily_seasonality=False,\n",
    "    weekly_seasonality=True,\n",
    "    yearly_seasonality=True,\n",
    "    seasonality_mode='multiplicative'\n",
    "    )\n",
    "  \n",
    "  # train the model\n",
    "  model.fit( history_pd )\n",
    "  \n",
    "  # make predictions\n",
    "  future_pd = model.make_future_dataframe(\n",
    "    periods=365, \n",
    "    freq='d', \n",
    "    include_history=True\n",
    "    )\n",
    "  forecast_pd = model.predict( future_pd )  \n",
    "\n",
    "  # get relevant fields from forecast\n",
    "  f_pd = forecast_pd[ ['ds','yhat', 'yhat_upper', 'yhat_lower'] ].set_index('ds')\n",
    "  \n",
    "  # get relevant fields from history\n",
    "  h_pd = history_pd[['ds','id','y']].set_index('ds')\n",
    "  \n",
    "  # join history and forecast\n",
    "  results_pd = f_pd.join( h_pd, how='left' )\n",
    "  results_pd.reset_index(level=0, inplace=True)\n",
    "  \n",
    "  # get store & item from incoming data set\n",
    "  results_pd['id'] = history_pd['id'].iloc[0]\n",
    "  # --------------------------------------\n",
    "  \n",
    "  # return expected dataset\n",
    "  return results_pd[ ['ds', 'id', 'y', 'yhat', 'yhat_upper', 'yhat_lower'] ]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15e3aa20-b6ad-47d1-bc82-51767acc1240",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date\n",
    " \n",
    "results = (\n",
    "  id_history\n",
    "    .groupBy('id')\n",
    "      .applyInPandas(forecast, schema=result_schema)\n",
    "    .withColumn('training_date', current_date() )\n",
    "    )\n",
    "\n",
    "results.write.mode(\"overwrite\").saveAsTable(\"forecast\")\n",
    "\n",
    "display(results)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6639774240803747,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Prophet",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
