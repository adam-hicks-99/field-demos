{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cca00bd5-7c5f-4161-94bf-b5a545072442",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import random\n",
    "import pandas as pd\n",
    "from mlflow.models.signature import infer_signature\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "from mlflow.tracking import MlflowClient\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class DummyModel(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, model_id, coefficient, bias, accuracy):\n",
    "        self.model_id = model_id\n",
    "        self.coefficient = coefficient\n",
    "        self.bias = bias\n",
    "        self.accuracy = accuracy\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        # Simple linear transformation: output = coefficient * input + bias\n",
    "        return [self.coefficient * x + self.bias for x in model_input]\n",
    "\n",
    "def train_dummy_model_for_model_id(model_id):\n",
    "    \"\"\"\n",
    "    Trains a dummy model for a given model_id, logs it with MLflow,\n",
    "    sets a description on the model version based on version (using the MLflow description methods), and \n",
    "    appends run details (including the UC model location) to the master table.\n",
    "    \"\"\"\n",
    "    # Generate dummy parameters and accuracy.\n",
    "    coefficient = random.uniform(0.5, 1.5)\n",
    "    bias = random.uniform(-1, 1)\n",
    "    accuracy = random.uniform(0.7, 0.8)\n",
    "    \n",
    "    model = DummyModel(model_id, coefficient, bias, accuracy)\n",
    "    \n",
    "    # Create an example input DataFrame for signature inference.\n",
    "    example_input = pd.DataFrame({\"x\": [0, 1, 2, 3, 4, 5]})\n",
    "    example_output = model.predict(None, example_input[\"x\"].tolist())\n",
    "    signature = infer_signature(example_input, example_output)\n",
    "    \n",
    "    artifact_path = f\"dummy_model_{model_id}\"\n",
    "    registered_model_name = model_id\n",
    "    \n",
    "    with mlflow.start_run() as run:\n",
    "        mlflow.log_param(\"model_id\", model_id)\n",
    "        mlflow.log_param(\"coefficient\", coefficient)\n",
    "        mlflow.log_param(\"bias\", bias)\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        \n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=artifact_path,\n",
    "            python_model=model,\n",
    "            signature=signature,\n",
    "            registered_model_name=registered_model_name\n",
    "        )\n",
    "        \n",
    "        run_id = run.info.run_id\n",
    "        print(f\"Logged DummyModel for model_id '{model_id}' with run ID: {run_id} and accuracy: {accuracy:.3f}\")\n",
    "        \n",
    "        client = MlflowClient()\n",
    "        # Retrieve all versions of this registered model.\n",
    "        model_versions = client.search_model_versions(f\"name='{registered_model_name}'\")\n",
    "        new_version = max([int(mv.version) for mv in model_versions])\n",
    "        \n",
    "        # Instead of setting an alias, update the model version's description.\n",
    "        if len(model_versions) == 1:\n",
    "            description = \"prod\"\n",
    "        else:\n",
    "            description = f\"challenger_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "        \n",
    "        client.update_model_version(\n",
    "            name=registered_model_name,\n",
    "            version=str(new_version),\n",
    "            description=description\n",
    "        )\n",
    "        print(f\"Set description for model '{registered_model_name}' version {new_version} to '{description}'.\")\n",
    "        \n",
    "        # Build the UC model location (URI) that can be used to load the model.\n",
    "        # Note: The URI no longer uses alias but can refer to the version with description.\n",
    "        model_location = f\"models:/{registered_model_name}/{new_version}\"\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce330199-435f-45b3-a71b-eb0ff42c2999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List of model_ids to track the definition of different models.\n",
    "model_ids = [\n",
    "    \"field_demos.ml_ops.DummyModel_churn_prediction\",\n",
    "    \"field_demos.ml_ops.DummyModel_fraud_detection\",\n",
    "    \"field_demos.ml_ops.DummyModel_customer_segmentation\",\n",
    "    \"field_demos.ml_ops.DummyModel_demand_forecasting\",\n",
    "    \"field_demos.ml_ops.DummyModel_product_recommendation\"\n",
    "]\n",
    "\n",
    "# Ensure MLflow uses the Databricks Unity Catalog registry.\n",
    "mlflow.tracking._model_registry.utils._get_registry_uri_from_spark_session = lambda: \"databricks-uc\"\n",
    "\n",
    "# Train and log a dummy model for each model_id.\n",
    "models = {}\n",
    "for model_id in model_ids:\n",
    "    models[model_id] = train_dummy_model_for_model_id(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dae51828-77e7-46fe-b546-83119d4b6d05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_master_model_log():\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    import mlflow\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, DoubleType, BooleanType, IntegerType\n",
    "\n",
    "    # Set the registry URI for Unity Catalog\n",
    "    mlflow.tracking._model_registry.utils._get_registry_uri_from_spark_session = lambda: \"databricks-uc\"\n",
    "\n",
    "    # Initialize the MLflow client\n",
    "    client = MlflowClient()\n",
    "\n",
    "    # List to collect results for the master model log (version-level)\n",
    "    results = []\n",
    "\n",
    "    # Search for all registered models\n",
    "    registered_models = client.search_registered_models()\n",
    "\n",
    "    # Iterate over models and filter for those in the field_demos.ml_ops namespace\n",
    "    for model in registered_models:\n",
    "        if model.name.startswith(\"field_demos.ml_ops\"):\n",
    "            # Retrieve all versions of this model\n",
    "            model_versions = client.search_model_versions(f\"name='{model.name}'\")\n",
    "            if not model_versions:\n",
    "                results.append({\n",
    "                    \"model_name\": model.name,\n",
    "                    \"version\": None,\n",
    "                    \"run_id\": None,\n",
    "                    \"accuracy\": None,\n",
    "                    \"description\": None\n",
    "                })\n",
    "            else:\n",
    "                for version in model_versions:\n",
    "                    # Retrieve run details to extract metrics\n",
    "                    run = client.get_run(version.run_id)\n",
    "                    # Get the accuracy metric (returns None if not found)\n",
    "                    accuracy = run.data.metrics.get(\"accuracy\")\n",
    "                    results.append({\n",
    "                        \"model_name\": model.name,\n",
    "                        \"version\": version.version,\n",
    "                        \"run_id\": version.run_id,\n",
    "                        \"accuracy\": float(accuracy) if accuracy is not None else None,\n",
    "                        \"description\": version.description\n",
    "                    })\n",
    "\n",
    "    # Create the master model log Delta table\n",
    "    master_schema = StructType([\n",
    "        StructField(\"model_name\", StringType(), True),\n",
    "        StructField(\"version\", StringType(), True),\n",
    "        StructField(\"run_id\", StringType(), True),\n",
    "        StructField(\"accuracy\", DoubleType(), True),\n",
    "        StructField(\"description\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    master_df = spark.createDataFrame(results, schema=master_schema)\n",
    "    master_df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(\"field_demos.ml_ops.master_model_log\")\n",
    "\n",
    "    print(\"Master model log successfully written to Delta table 'field_demos.ml_ops.master_model_log'.\")\n",
    "\n",
    "    # Build the model-level report with additional challenger info\n",
    "    # For each model we track:\n",
    "    # - number_of_versions: count of versions\n",
    "    # - has_prod_model: True if any version has description \"prod\"\n",
    "    # - max_prod_accuracy: highest accuracy among prod versions\n",
    "    # - best_challenger: best non-prod record (dict with keys: accuracy, version) encountered\n",
    "    report_dict = {}\n",
    "    for record in results:\n",
    "        model_name = record[\"model_name\"]\n",
    "        if model_name not in report_dict:\n",
    "            report_dict[model_name] = {\n",
    "                \"number_of_versions\": 0,\n",
    "                \"has_prod_model\": False,\n",
    "                \"max_prod_accuracy\": None,   # Highest accuracy among prod versions\n",
    "                \"best_challenger\": None      # Store best challenger record for non-prod\n",
    "            }\n",
    "        report_dict[model_name][\"number_of_versions\"] += 1\n",
    "\n",
    "        # Check for prod version\n",
    "        if record[\"description\"] == \"prod\":\n",
    "            report_dict[model_name][\"has_prod_model\"] = True\n",
    "            if record[\"accuracy\"] is not None:\n",
    "                current_max = report_dict[model_name][\"max_prod_accuracy\"]\n",
    "                if current_max is None or record[\"accuracy\"] > current_max:\n",
    "                    report_dict[model_name][\"max_prod_accuracy\"] = record[\"accuracy\"]\n",
    "        else:\n",
    "            # For non-prod versions, update best challenger record if applicable\n",
    "            if record[\"accuracy\"] is not None:\n",
    "                best_challenger = report_dict[model_name][\"best_challenger\"]\n",
    "                if best_challenger is None or record[\"accuracy\"] > best_challenger[\"accuracy\"]:\n",
    "                    report_dict[model_name][\"best_challenger\"] = {\n",
    "                        \"accuracy\": record[\"accuracy\"],\n",
    "                        \"version\": record[\"version\"]\n",
    "                    }\n",
    "\n",
    "    # Create the report list from the aggregated dictionary\n",
    "    report_results = []\n",
    "    for model_name, agg in report_dict.items():\n",
    "        has_prod = agg[\"has_prod_model\"]\n",
    "        max_prod_accuracy = agg[\"max_prod_accuracy\"]\n",
    "        best_challenger = agg[\"best_challenger\"]\n",
    "\n",
    "        # Determine if retraining is needed\n",
    "        if has_prod:\n",
    "            needs_retrained = (max_prod_accuracy is None) or (max_prod_accuracy < 0.75)\n",
    "        else:\n",
    "            needs_retrained = True\n",
    "\n",
    "        # New logic: needs_inspected is true if a non-prod (challenger) exists and its accuracy is higher than the prod accuracy.\n",
    "        if has_prod and best_challenger is not None and max_prod_accuracy is not None and best_challenger[\"accuracy\"] > max_prod_accuracy:\n",
    "            needs_inspected = True\n",
    "            challenger_accuracy = best_challenger[\"accuracy\"]\n",
    "            challenger_version = best_challenger[\"version\"]\n",
    "            prod_accuracy_col = max_prod_accuracy\n",
    "        else:\n",
    "            needs_inspected = False\n",
    "            challenger_accuracy = None\n",
    "            challenger_version = None\n",
    "            prod_accuracy_col = max_prod_accuracy\n",
    "\n",
    "        report_results.append({\n",
    "            \"model_name\": model_name,\n",
    "            \"number_of_versions\": agg[\"number_of_versions\"],\n",
    "            \"has_prod_model\": has_prod,\n",
    "            \"needs_retrained\": needs_retrained,\n",
    "            \"needs_inspected\": needs_inspected,\n",
    "            \"prod_accuracy\": prod_accuracy_col,\n",
    "            \"challenger_accuracy\": challenger_accuracy,\n",
    "            \"challenger_version\": challenger_version\n",
    "        })\n",
    "\n",
    "    # Create the report Delta table with the additional challenger version column\n",
    "    report_schema = StructType([\n",
    "        StructField(\"model_name\", StringType(), True),\n",
    "        StructField(\"number_of_versions\", IntegerType(), True),\n",
    "        StructField(\"has_prod_model\", BooleanType(), True),\n",
    "        StructField(\"needs_retrained\", BooleanType(), True),\n",
    "        StructField(\"needs_inspected\", BooleanType(), True),\n",
    "        StructField(\"prod_accuracy\", DoubleType(), True),\n",
    "        StructField(\"challenger_accuracy\", DoubleType(), True),\n",
    "        StructField(\"challenger_version\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    report_df = spark.createDataFrame(report_results, schema=report_schema)\n",
    "    report_df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(\"field_demos.ml_ops.master_model_report\")\n",
    "\n",
    "    print(\"Model report successfully written to Delta table 'field_demos.ml_ops.master_model_report'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3354f0a6-910b-4851-97b5-f10ef01b754c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "generate_master_model_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24eaf138-f799-4374-9cb9-2ee55392a405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from field_demos.ml_ops.master_model_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "222fbdda-e60c-4284-98e4-5817a8bd2cbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from field_demos.ml_ops.master_model_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fd03e32-a528-4d9f-b1f1-0931a1d9f33e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def automated_retrain():\n",
    "    import datetime\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "    from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "    # Placeholder retrain function. Replace this with your actual retraining logic.\n",
    "    def retrain_model(model_name):\n",
    "        print(f\"Retraining model '{model_name}'...\")\n",
    "        train_dummy_model_for_model_id(model_name)\n",
    "        return f\"Success\"\n",
    "\n",
    "    # Read the model-level report table\n",
    "    try:\n",
    "        report_df = spark.table(\"field_demos.ml_ops.master_model_report\")\n",
    "    except Exception as e:\n",
    "        print(\"Error reading master_model_report table:\", e)\n",
    "        return\n",
    "\n",
    "    # Filter models that need retraining\n",
    "    models_to_retrain = report_df.filter(\"needs_retrained = true\").collect()\n",
    "\n",
    "    # List to collect retraining logs\n",
    "    retrain_logs = []\n",
    "\n",
    "    for row in models_to_retrain:\n",
    "        model_name = row[\"model_name\"]\n",
    "        # Call the retraining function\n",
    "        status = retrain_model(model_name)\n",
    "        # Record the retraining event\n",
    "        retrain_logs.append({\n",
    "            \"model_name\": model_name,\n",
    "            \"retrain_time\": datetime.datetime.now(),\n",
    "            \"status\": status,\n",
    "            \"details\": f\"Retraining executed for model {model_name}.\"\n",
    "        })\n",
    "\n",
    "    if retrain_logs:\n",
    "        # Define the schema for the retrain log table\n",
    "        log_schema = StructType([\n",
    "            StructField(\"model_name\", StringType(), True),\n",
    "            StructField(\"retrain_time\", TimestampType(), True),\n",
    "            StructField(\"status\", StringType(), True),\n",
    "            StructField(\"details\", StringType(), True)\n",
    "        ])\n",
    "\n",
    "        # Create a DataFrame from the logs\n",
    "        log_df = spark.createDataFrame(retrain_logs, schema=log_schema)\n",
    "\n",
    "        # Write logs to the Delta table (append mode)\n",
    "        log_df.write.format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .saveAsTable(\"field_demos.ml_ops.model_retrain_log\")\n",
    "\n",
    "        print(\"Retraining events logged to Delta table 'field_demos.ml_ops.model_retrain_log'.\")\n",
    "    else:\n",
    "        print(\"No models required retraining.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a0226f8-340a-462d-a096-6acaaf4eeab6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "automated_retrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e699f6b-b2cd-40d0-aae7-eb4562bda7da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "generate_master_model_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "902c34c6-af7d-4eab-a06d-39deb3e1e3cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from field_demos.ml_ops.master_model_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8e5e4af-4462-40b0-8854-d2a0bc7ee34a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from field_demos.ml_ops.master_model_report"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8573087356888513,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Multi Model Management",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
