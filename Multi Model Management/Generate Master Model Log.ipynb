{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6882bf3d-4028-48f2-a582-8762986cfe36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from mlflow.tracking import MlflowClient\n",
    "import mlflow\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, BooleanType, IntegerType\n",
    "\n",
    "# Set the registry URI for Unity Catalog\n",
    "mlflow.tracking._model_registry.utils._get_registry_uri_from_spark_session = lambda: \"databricks-uc\"\n",
    "\n",
    "# Initialize the MLflow client\n",
    "client = MlflowClient()\n",
    "\n",
    "# List to collect results for the master model log (version-level)\n",
    "results = []\n",
    "\n",
    "# Search for all registered models\n",
    "registered_models = client.search_registered_models()\n",
    "\n",
    "# Iterate over models and filter for those in the field_demos.ml_ops namespace\n",
    "for model in registered_models:\n",
    "    if model.name.startswith(\"field_demos.ml_ops\"):\n",
    "        # Retrieve all versions of this model\n",
    "        model_versions = client.search_model_versions(f\"name='{model.name}'\")\n",
    "        if not model_versions:\n",
    "            results.append({\n",
    "                \"model_name\": model.name,\n",
    "                \"version\": None,\n",
    "                \"run_id\": None,\n",
    "                \"accuracy\": None,\n",
    "                \"description\": None\n",
    "            })\n",
    "        else:\n",
    "            for version in model_versions:\n",
    "                # Retrieve run details to extract metrics\n",
    "                run = client.get_run(version.run_id)\n",
    "                # Get the accuracy metric (returns None if not found)\n",
    "                accuracy = run.data.metrics.get(\"accuracy\")\n",
    "                results.append({\n",
    "                    \"model_name\": model.name,\n",
    "                    \"version\": version.version,\n",
    "                    \"run_id\": version.run_id,\n",
    "                    \"accuracy\": float(accuracy) if accuracy is not None else None,\n",
    "                    \"description\": version.description\n",
    "                })\n",
    "\n",
    "# Create the master model log Delta table\n",
    "master_schema = StructType([\n",
    "    StructField(\"model_name\", StringType(), True),\n",
    "    StructField(\"version\", StringType(), True),\n",
    "    StructField(\"run_id\", StringType(), True),\n",
    "    StructField(\"accuracy\", DoubleType(), True),\n",
    "    StructField(\"description\", StringType(), True)\n",
    "])\n",
    "\n",
    "master_df = spark.createDataFrame(results, schema=master_schema)\n",
    "master_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"field_demos.ml_ops.master_model_log\")\n",
    "\n",
    "print(\"Master model log successfully written to Delta table 'field_demos.ml_ops.master_model_log'.\")\n",
    "\n",
    "# Build the model-level report with additional challenger info\n",
    "# For each model we track:\n",
    "# - number_of_versions: count of versions\n",
    "# - has_prod_model: True if any version has description \"prod\"\n",
    "# - max_prod_accuracy: highest accuracy among prod versions\n",
    "# - best_challenger: best non-prod record (dict with keys: accuracy, version) encountered\n",
    "report_dict = {}\n",
    "for record in results:\n",
    "    model_name = record[\"model_name\"]\n",
    "    if model_name not in report_dict:\n",
    "        report_dict[model_name] = {\n",
    "            \"number_of_versions\": 0,\n",
    "            \"has_prod_model\": False,\n",
    "            \"max_prod_accuracy\": None,   # Highest accuracy among prod versions\n",
    "            \"best_challenger\": None      # Store best challenger record for non-prod\n",
    "        }\n",
    "    report_dict[model_name][\"number_of_versions\"] += 1\n",
    "\n",
    "    # Check for prod version\n",
    "    if record[\"description\"] == \"prod\":\n",
    "        report_dict[model_name][\"has_prod_model\"] = True\n",
    "        if record[\"accuracy\"] is not None:\n",
    "            current_max = report_dict[model_name][\"max_prod_accuracy\"]\n",
    "            if current_max is None or record[\"accuracy\"] > current_max:\n",
    "                report_dict[model_name][\"max_prod_accuracy\"] = record[\"accuracy\"]\n",
    "    else:\n",
    "        # For non-prod versions, update best challenger record if applicable\n",
    "        if record[\"accuracy\"] is not None:\n",
    "            best_challenger = report_dict[model_name][\"best_challenger\"]\n",
    "            if best_challenger is None or record[\"accuracy\"] > best_challenger[\"accuracy\"]:\n",
    "                report_dict[model_name][\"best_challenger\"] = {\n",
    "                    \"accuracy\": record[\"accuracy\"],\n",
    "                    \"version\": record[\"version\"]\n",
    "                }\n",
    "\n",
    "# Create the report list from the aggregated dictionary\n",
    "report_results = []\n",
    "for model_name, agg in report_dict.items():\n",
    "    has_prod = agg[\"has_prod_model\"]\n",
    "    max_prod_accuracy = agg[\"max_prod_accuracy\"]\n",
    "    best_challenger = agg[\"best_challenger\"]\n",
    "\n",
    "    # Determine if retraining is needed\n",
    "    if has_prod:\n",
    "        needs_retrained = (max_prod_accuracy is None) or (max_prod_accuracy < 0.75)\n",
    "    else:\n",
    "        needs_retrained = True\n",
    "\n",
    "    # New logic: needs_inspected is true if a non-prod (challenger) exists and its accuracy is higher than the prod accuracy.\n",
    "    if has_prod and best_challenger is not None and max_prod_accuracy is not None and best_challenger[\"accuracy\"] > max_prod_accuracy:\n",
    "        needs_inspected = True\n",
    "        challenger_accuracy = best_challenger[\"accuracy\"]\n",
    "        challenger_version = best_challenger[\"version\"]\n",
    "        prod_accuracy_col = max_prod_accuracy\n",
    "    else:\n",
    "        needs_inspected = False\n",
    "        challenger_accuracy = None\n",
    "        challenger_version = None\n",
    "        prod_accuracy_col = max_prod_accuracy\n",
    "\n",
    "    report_results.append({\n",
    "        \"model_name\": model_name,\n",
    "        \"number_of_versions\": agg[\"number_of_versions\"],\n",
    "        \"has_prod_model\": has_prod,\n",
    "        \"needs_retrained\": needs_retrained,\n",
    "        \"needs_inspected\": needs_inspected,\n",
    "        \"prod_accuracy\": prod_accuracy_col,\n",
    "        \"challenger_accuracy\": challenger_accuracy,\n",
    "        \"challenger_version\": challenger_version\n",
    "    })\n",
    "\n",
    "# Create the report Delta table with the additional challenger version column\n",
    "report_schema = StructType([\n",
    "    StructField(\"model_name\", StringType(), True),\n",
    "    StructField(\"number_of_versions\", IntegerType(), True),\n",
    "    StructField(\"has_prod_model\", BooleanType(), True),\n",
    "    StructField(\"needs_retrained\", BooleanType(), True),\n",
    "    StructField(\"needs_inspected\", BooleanType(), True),\n",
    "    StructField(\"prod_accuracy\", DoubleType(), True),\n",
    "    StructField(\"challenger_accuracy\", DoubleType(), True),\n",
    "    StructField(\"challenger_version\", StringType(), True)\n",
    "])\n",
    "\n",
    "report_df = spark.createDataFrame(report_results, schema=report_schema)\n",
    "report_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"field_demos.ml_ops.master_model_report\")\n",
    "\n",
    "print(\"Model report successfully written to Delta table 'field_demos.ml_ops.master_model_report'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11ddf5e2-6c8b-41d2-8304-4ae279239f4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Generate Master Model Log",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
